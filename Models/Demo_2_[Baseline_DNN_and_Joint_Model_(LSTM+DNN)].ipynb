{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo 2 [Baseline DNN and Joint Model (LSTM+DNN)]",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "nBHh4DetpWnL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Set Up"
      ]
    },
    {
      "metadata": {
        "id": "XHRZZv3YzUmv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install numpy pandas gensim tqdm nltk pandas_ml imblearn tensorboardcolab\n",
        "!pip3 install scikit-learn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U9TDcqluMAn0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "pd.options.mode.chained_assignment = None\n",
        "import numpy as np \n",
        "from copy import deepcopy\n",
        "from string import punctuation\n",
        "\n",
        "import glob\n",
        "import json\n",
        "\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec \n",
        "TaggedDocument = gensim.models.doc2vec.TaggedDocument \n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer \n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from random import randint\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import random\n",
        "import pandas_ml as pdml\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "he96WMzfLujH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Preprocessing Steps:\n",
        "\n",
        "\n",
        "1.  Load Dataset (typhoon tweets, sentiment140)\n",
        "2.  Build Word Embedding Model or Load Pre-trained Model\n",
        "3.  Create Sentiment Features for Feature Extractor Model (LSTM)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "oRb6-vr-NFJP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Helper functions for preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "FWYTaR9KNJ-5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#loading sentiment140 dataset and project (tweets text,label). It also maps labels (4:1) in the original dataset.\n",
        "def ingest():\n",
        "    data = pd.read_csv('./Sentiment140/training.1600000.processed.noemoticon.csv', encoding = \"ISO-8859-1\") # use your path\n",
        "    data.drop(['ItemID', 'SentimentSource'], axis=1, inplace=True)\n",
        "    data = data[data.Sentiment.isnull() == False]\n",
        "    data['Sentiment'] = data['Sentiment'].map( {4:1,0:0})\n",
        "    data = data[data['SentimentText'].isnull() == False]\n",
        "    data.reset_index(inplace=True)\n",
        "    data.drop('index', axis=1, inplace=True)    \n",
        "    data=shuffle(data) #randmoize sequence of data\n",
        "    print(('dataset loaded with shape', data.shape))    \n",
        "\n",
        "    return data\n",
        "\n",
        "# extract tweets text and label; also it maps label (4) to (1) as a positive \n",
        "def postprocess(data, n=1600000): # loading 1.6 million tweets\n",
        "    data = data.head(n)\n",
        "    data['tokens'] = data['SentimentText'].progress_map(tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n",
        "    data = data[data.tokens != 'NC']\n",
        "    data.reset_index(inplace=True)\n",
        "    data.drop('index', inplace=True, axis=1)\n",
        "    return data\n",
        "\n",
        "#tokenizing tweets: clean hashtags,usernames, and stop words. return list of words\n",
        "def tokenize(tweet):\n",
        "    try:\n",
        "        tokens = tokenizer.tokenize(tweet.lower())\n",
        "        tokens = list([t for t in tokens if not t.startswith('@')])\n",
        "        tokens = list([t for t in tokens if not t.startswith('#')])\n",
        "        tokens = list([t for t in tokens if not t.startswith('http')])\n",
        "        tokens = list([t for t in tokens if not t.startswith('https')])\n",
        "        \n",
        "        #keep only text tweets, ignore numbers\n",
        "        tokens=list([t for t in tokens if t.isalpha()])\n",
        "        \n",
        "        return tokens\n",
        "    except:\n",
        "        return 'NC'\n",
        "\n",
        "# build a labeledSentence from tweet's text to train the word embedding model.\n",
        "def labelizeTweets(tweets, label_type):\n",
        "    labelized = []\n",
        "    for i,v in tqdm(enumerate(tweets)):\n",
        "        label = '%s_%s'%(label_type,i)\n",
        "        labelized.append(TaggedDocument(v, [label]))\n",
        "    return labelized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f6dWr8mV64HM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 Load Sentiment140 Dataset"
      ]
    },
    {
      "metadata": {
        "id": "SktQ15pd62hc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentiment140=ingest() # loading 1.6 Million Labelled tweets\n",
        "sentiment140=postprocess(sentiment140) # clearning and representing data as tweet and sentiment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uBjdduM07MB9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# separate Training Features (X) from Labels (Y)\n",
        "\n",
        "n=1600000 # data size 1.6 million tweets\n",
        "x_sentiment,y_sentiment = np.array(sentiment140.head(n).tokens),np.array(sentiment140.head(n).Sentiment) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cnu75Ekq9NcJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 Load Typhoon Tweets"
      ]
    },
    {
      "metadata": {
        "id": "eErYwXiL9RNZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "typhoon_df=pd.DataFrame()\n",
        "path ='./Dataset/Typhoons_tweets' # use your path\n",
        "allFiles = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "# merge all typhoon tweets into one file\n",
        "list_ = []\n",
        "for file_ in allFiles:\n",
        "    df = pd.read_csv(file_,index_col=None, header=0)\n",
        "    list_.append(df) # append all tweets_lists into one list\n",
        "    \n",
        "typhoon_df = pd.concat(list_) # merage all tweets together.\n",
        "typhoon_tweets=typhoon_df['text'].tolist()\n",
        "\n",
        "tweetsTokens=list()\n",
        "\n",
        "#Tweets preprocessing\n",
        "for tweet in tqdm(typhoon_tweets):\n",
        "    tweet=str(tweet)\n",
        "    tweetsTokens.append(tokenize(tweet)) # clean tweets and append to tweets_list\n",
        "\n",
        "typhoon_tweets_tokens=tweetsTokens # tokenized tweets."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8RtdnXh47_xG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Build Word Embedding Model.\n",
        "\n",
        "To train a word embedding model -from scratch- RUN the first cell.\n",
        "\n",
        "OR\n",
        "\n",
        "Load a pre-trained word embedding model.\n"
      ]
    },
    {
      "metadata": {
        "id": "d60XsZzE8FJF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ---------------------------- Build A Word Embedding Model (Word2vec) ----------------------- #\n",
        "\n",
        "# combine all twees together. \n",
        "allTweets=sentiment140['tokens'].tolist()+typhoon_tweets_tokens\n",
        "\n",
        "# building all word embedding (Words)\n",
        "allTweets = labelizeTweets(allTweets, 'AllTWEETS')\n",
        "\n",
        "n_dim=200 # word2vec dimension\n",
        "\n",
        "word_emb = Word2Vec(size=n_dim, min_count=10,sg=1) #sg=1 Skipgram is used\n",
        "word_emb.build_vocab([x.words for x in tqdm(allTweets)]) # words attribute by LabeledSentence\n",
        "word_emb.train([x.words for x in tqdm(allTweets)],total_examples=len(allTweets),epochs=10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CcJxiZqh8Fbl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ----------------------- Load a Pre-trained Word Embedding Model -----------------#\n",
        "word_emb=Word2Vec.load('./wordEmbedding.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "diqE5rIOAa66",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train and Evaluate Baseline (DNN)"
      ]
    },
    {
      "metadata": {
        "id": "jaCOivRuL82P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Pre-processed Training and Test dataset"
      ]
    },
    {
      "metadata": {
        "id": "Mem-aVLsBzbb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# n: number of batches in training dataset. \n",
        "# In our preprocessed data, each batch has 32 typhoon sample. Total batches is 79\n",
        "\n",
        "def load_training_Batches(n=79): \n",
        "    typhoon_features=[]\n",
        "    labels=[]\n",
        "            \n",
        "    for i in range(n):\n",
        "        \n",
        "        typhoon_features.append(np.load('./Training Data-Baseline/typhoon_features/typhoon_features'+str(i)+'.npy'))\n",
        "        labels.append(np.int_(np.load('./Training Data-Baseline/labels/labels'+str(i)+'.npy')))\n",
        "\n",
        "    return typhoon_features,labels\n",
        "  \n",
        "# In testing data, there is 19 batches of testing data. Each batch has 32 typhoon sample  \n",
        "def load_testing_Batches(n=19): \n",
        "    typhoon_features=[]\n",
        "    labels=[]\n",
        "\n",
        "    for i in range(n):\n",
        "        \n",
        "        typhoon_features.append(np.load('./Testing Data-Baseline/typhoon_features/typhoon_features'+str(i)+'.npy'))\n",
        "        labels.append(np.int_(np.load('./Testing Data-Baseline/labels/labels'+str(i)+'.npy')))\n",
        "            \n",
        "    return typhoon_features,labels\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hKpzLTn5R8lh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_features,training_labels=load_training_Batches()\n",
        "testing_features,testing_labels=load_testing_Batches()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oI4jDlKGDhiK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## DNN Baseline Model"
      ]
    },
    {
      "metadata": {
        "id": "NNJZXltlDlD6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### DNN Model Architecture"
      ]
    },
    {
      "metadata": {
        "id": "e2SjymrKDnIw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "typhoon_batchSize=32\n",
        "featuresCount=12\n",
        "typhoon_classes=4\n",
        "hidden_nodes=16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ddwva-hsS17h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def DNN_model(X_data): \n",
        "    \n",
        "    weights = {\n",
        "        'h1': tf.Variable(name='weight_h1',initial_value=tf.random_normal([featuresCount, hidden_nodes])),\n",
        "        'h2': tf.Variable(name='weight_h2',initial_value=tf.random_normal([hidden_nodes, hidden_nodes])),\n",
        "        'out': tf.Variable(name='weight_out',initial_value=tf.random_normal([hidden_nodes, typhoon_classes]))\n",
        "    }\n",
        "    # initialize bias\n",
        "    biases = {\n",
        "        'b1': tf.Variable(name='hidden_bias1',initial_value=tf.random_normal([hidden_nodes])),\n",
        "        'b2': tf.Variable(name='hidden_bias2',initial_value=tf.random_normal([hidden_nodes])),\n",
        "        'out': tf.Variable(name='out_bias',initial_value=tf.random_normal([typhoon_classes]))\n",
        "    }\n",
        "    \n",
        "    layer_1 = tf.add(tf.matmul(X_data, weights['h1']), biases['b1'])    \n",
        "    layer_1 = tf.layers.dense(inputs=layer_1,activation=tf.nn.relu,units=hidden_nodes)\n",
        "    #layer_1 = tf.contrib.layers.batch_norm(layer_1, center=True, scale=True, is_training=phase, scope='bn1')\n",
        "    layer_1 = tf.nn.dropout(layer_1, 0.6)\n",
        "\n",
        "    \n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    layer_2 = tf.layers.dense(inputs=layer_2,activation=tf.nn.relu,units=hidden_nodes)\n",
        "    #layer_2 = tf.contrib.layers.batch_norm(layer_2, center=True, scale=True, is_training=phase, scope='bn2')\n",
        "    layer_2= tf.nn.dropout(layer_2, 0.75)\n",
        "    \n",
        "    Ylogits=(tf.matmul(layer_2,weights['out'])+biases['out'])\n",
        "    y_pred = tf.nn.softmax(Ylogits)\n",
        "\n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rlhdq4G0TGq6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Computation Graph"
      ]
    },
    {
      "metadata": {
        "id": "1sBeNrd3TJ0z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "DNN_Graph=tf.Graph()\n",
        "grad_clip_margin=4\n",
        "\n",
        "with DNN_Graph.as_default():\n",
        "    \n",
        "    #prameters for DNN Model\n",
        "    typhoon_X=tf.placeholder(name='Typhoon_Input',dtype=tf.float32,shape=[typhoon_batchSize,featuresCount])\n",
        "    typhoon_ytrue=tf.placeholder(name='Typhoon_Label',dtype=tf.float32,shape=[typhoon_batchSize,typhoon_classes])\n",
        "    \n",
        "    with tf.name_scope(\"DNN_Model\") as scope:\n",
        "        typhoon_ypred=DNN_model(typhoon_X)\n",
        "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    \n",
        "        with tf.control_dependencies(update_ops):\n",
        "            # Ensures that we execute the update_ops before performing the train_step\n",
        "            dnn_loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "                logits=typhoon_ypred,labels=typhoon_ytrue))\n",
        "                                        \n",
        "            # Cliping the gradient loss\n",
        "            gradients = tf.gradients(dnn_loss, tf.trainable_variables())\n",
        "            clipper_, _ = tf.clip_by_global_norm(gradients, grad_clip_margin)\n",
        "            \n",
        "            optimizer = tf.train.AdamOptimizer(0.001)\n",
        "            typhoon_opt = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
        "            \n",
        "            predict_ = tf.equal(tf.argmax(typhoon_ypred,1), tf.argmax(typhoon_ytrue,1))\n",
        "\n",
        "            dnn_accuracy = tf.reduce_mean(tf.cast(predict_, tf.float32))\n",
        "            \n",
        "            y_p=tf.argmax(typhoon_ypred,1)            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JBapV7XCTec7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualizing Training and Testing in Tensorboard"
      ]
    },
    {
      "metadata": {
        "id": "1ChDPvgoTdVb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "\n",
        "tbc=TensorBoardColab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dM0GXNmxDn5x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Execution Session"
      ]
    },
    {
      "metadata": {
        "id": "hPfE5jURDtF5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logdir1 = \"./Graph/DNN_Model(training)/\"\n",
        "logdir2 = \"./Graph/DNN_Model(testing)/\"\n",
        "\n",
        "epochs=100\n",
        "training_iteration=79 \n",
        "testing_iteration=19\n",
        "\n",
        "with tf.Session(graph=DNN_Graph) as sess:\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())   \n",
        "    writer1 = tf.summary.FileWriter(logdir1, sess.graph)\n",
        "    writer2 = tf.summary.FileWriter(logdir2, sess.graph)\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    for j in range(epochs):\n",
        "        \n",
        "        accuracy=[]\n",
        "\n",
        "                \n",
        "        for i in range(training_iteration):\n",
        "                                    \n",
        "            feed_dict={typhoon_X:training_features[i], typhoon_ytrue:training_labels[i]}\n",
        "            \n",
        "            sess.run([typhoon_opt],feed_dict=feed_dict)                    \n",
        "                        \n",
        "            typhoon_acc,dnn_loss_value=sess.run([dnn_accuracy,dnn_loss],feed_dict=feed_dict)\n",
        "            \n",
        "            accuracy.append(typhoon_acc)\n",
        "        \n",
        "        value = tf.Summary.Value(tag=\"Accuracy\",simple_value=np.mean(accuracy))\n",
        "        \n",
        "        summary1 = tf.Summary(value=[value])\n",
        "        writer1.add_summary(summary1,j)\n",
        "        \n",
        "        # Evaluating testdata\n",
        "        accuracy=[]  \n",
        "        precision=[]\n",
        "        recall=[]\n",
        "        f1=[]\n",
        "  \n",
        "\n",
        "        for i in range(testing_iteration):\n",
        "                                    \n",
        "            feed_dict={typhoon_X:testing_features[i], typhoon_ytrue:testing_labels[i]}                        \n",
        "            \n",
        "            typhoon_acc,typhoon_ypred_value=sess.run([dnn_accuracy,y_p],feed_dict=feed_dict)\n",
        "            \n",
        "            accuracy.append(typhoon_acc)\n",
        "\n",
        "            #metrics\n",
        "            y_true = np.argmax(testing_labels[i],1)\n",
        "            \n",
        "            accuracy.append(typhoon_acc)\n",
        "            precision.append(precision_score(y_true, typhoon_ypred_value,average='weighted'))\n",
        "            recall.append(recall_score(y_true, typhoon_ypred_value,average='weighted'))\n",
        "            f1.append(f1_score(y_true, typhoon_ypred_value,average='weighted'))\n",
        "                                                   \n",
        "        value = tf.Summary.Value(tag=\"Accuracy\",simple_value=np.mean(accuracy))\n",
        "        summary2 = tf.Summary(value=[value])\n",
        "        writer2.add_summary(summary2,j)\n",
        "              \n",
        "    writer1.close() \n",
        "    writer2.close() \n",
        "      \n",
        "      \n",
        "      # ----- Printing Average Performance Results After Training 100 Epoches------------------------# \n",
        "    print (\"Performance metric:\",np.mean(accuracy),np.mean(precision),np.mean(recall),np.mean(f1))        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "itm6wceYMHAO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluate Joint-Model (LSTM+DNN)\n"
      ]
    },
    {
      "metadata": {
        "id": "1D2LCv64Y8he",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Feature Extractor Model (LSTM)"
      ]
    },
    {
      "metadata": {
        "id": "bfPGVU9eMJP_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lstm_size = 64\n",
        "senti_batch_size = 48\n",
        "learning_rate = 0.001\n",
        "senti_classes=2 \n",
        "embed_size=200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fZ_YzQsMRe0R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def LSTM_Model(senti_inputs,transfer_learning=False):\n",
        "  \n",
        "    data=tf.nn.embedding_lookup(wordVectors,senti_inputs) # tweets_model_wordVectors, word_embedding_vectors, gloveVectors, wordVectors\n",
        "        \n",
        "    lstmCell=tf.nn.rnn_cell.LSTMCell(num_units=lstm_size,name='basic_lstm_cell')\n",
        "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
        "    outputs, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
        "\n",
        "    weight = tf.Variable(tf.truncated_normal([lstm_size, senti_classes]))\n",
        "    bias = tf.Variable(tf.constant(0.1, shape=[senti_classes]))\n",
        "    \n",
        "    value = tf.transpose(outputs, [1, 0, 2])\n",
        "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
        "    senti_logits = (tf.matmul(last, weight) + bias)\n",
        "    \n",
        "    senti_predictions=tf.nn.softmax(senti_logits)\n",
        "    \n",
        "    return senti_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fs6fcGvDZ3kx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1 Loading Data for Feature Extractor Model"
      ]
    },
    {
      "metadata": {
        "id": "vOaZCXHcaAXZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loading word embeddiing vocabs\n",
        "words=list(word_emb.wv.vocab)\n",
        "wordVectors=word_emb[word_emb.wv.vocab]\n",
        "\n",
        "maxSeqLength = 20 # based on average count of words per tweets in training dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_kmwsgHacuK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Buidling words indices.\n",
        "words_indices={} # from tweets2vec model\n",
        "\n",
        "i=0\n",
        "for w in words:\n",
        "  words_indices[w]=i\n",
        "  i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5GBw30p4ajzZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentiment_features = np.zeros((len(x_sentiment), maxSeqLength), dtype=int)\n",
        "\n",
        "i=0 # instance counter\n",
        "for instance in tqdm(x_sentiment):    \n",
        "    vectors=np.zeros(maxSeqLength,dtype=int)\n",
        "   \n",
        "    j=0  # word counter\n",
        "    for word in instance[:maxSeqLength]:\n",
        "        if word in words:\n",
        "            vectors[j]=words_indices[word] #get word index\n",
        "            j+=1\n",
        "    \n",
        "    sentiment_features[i]=vectors\n",
        "    i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L2xanyX5awbc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encode label classes into one-hot vector"
      ]
    },
    {
      "metadata": {
        "id": "ilIamtzMa39j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# representing class data as one-hot vectors\n",
        "y_sentiment_ = np.array([y_sentiment]).reshape(-1)\n",
        "one_hot_targets = np.eye(2)[y_sentiment_]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "beXUNlG3bFdb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3 Split Data (train-test, 80%-20%)"
      ]
    },
    {
      "metadata": {
        "id": "YlMJ8r1ebAI7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split data to train and test\n",
        "split_frac = 0.8\n",
        "split_idx = int(len(sentiment_features)*split_frac)\n",
        "\n",
        "senti_train_x, senti_val_x = sentiment_features[:split_idx], sentiment_features[split_idx:]\n",
        "senti_train_y, senti_val_y = one_hot_targets[:split_idx], one_hot_targets[split_idx:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N2FA8uk-bU_W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.4 Load Batches of Sentiment140 dataset"
      ]
    },
    {
      "metadata": {
        "id": "V-y84X2pbX1F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_Sentiment_Training_Batch():\n",
        "  \n",
        "    labels = np.zeros([senti_batch_size,senti_classes], dtype=int)\n",
        "    arr = np.zeros([senti_batch_size, maxSeqLength], dtype=int)\n",
        "    \n",
        "    for i in range(senti_batch_size):\n",
        "        num = randint(1,len(senti_train_x)-1) # get random index\n",
        "        \n",
        "        labels[i]=senti_train_y[num]\n",
        "        arr[i] = sentiment_features[num]\n",
        "    return arr, labels\n",
        "\n",
        "def load_Sentiment_Testing_Batch():\n",
        "  \n",
        "    labels = np.zeros([senti_batch_size,senti_classes], dtype=int)\n",
        "    arr = np.zeros([senti_batch_size, maxSeqLength],dtype=int)\n",
        "    \n",
        "    for i in range(senti_batch_size):\n",
        "        num = randint(1,len(senti_val_x)-1) # get random index\n",
        "        \n",
        "        labels[i]=senti_val_y[num]\n",
        "        arr[i] = sentiment_features[num]\n",
        "    \n",
        "    return arr, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1juCkQu-c3aM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Joint Model (LSTM+RNN) - Load Training and Testing dataset"
      ]
    },
    {
      "metadata": {
        "id": "7-fTDkMNdCmi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_training_Batches(n=79): \n",
        "    typhoon_features=[]\n",
        "    labels=[]\n",
        "    all_aggreTweets=[]\n",
        "    tweetsCounts=[]\n",
        "    \n",
        "    for i in range(n):\n",
        "        \n",
        "        typhoon_features.append(np.load('./Training Data-Joint Model/typhoon_features/typhoon_features'+str(i)+'.npy'))\n",
        "        labels.append(np.int_(np.load('./Training Data-Joint Model/labels/labels'+str(i)+'.npy')))\n",
        "        \n",
        "        all_aggreTweets.append(np.int_(np.load('./Training Data-Joint Model/all_aggreTweets/all_aggreTweets'+str(i)+'.npy')))\n",
        "        tweetsCounts.append(np.int_(np.load('./Training Data-Joint Model/tweetsCounts/tweetsCounts'+str(i)+'.npy')))\n",
        "    \n",
        "    return typhoon_features,labels,all_aggreTweets,tweetsCounts\n",
        "  \n",
        "  \n",
        "def load_testing_Batches(n=19): \n",
        "    typhoon_features=[]\n",
        "    labels=[]\n",
        "    all_aggreTweets=[]\n",
        "    tweetsCounts=[]\n",
        "    \n",
        "    for i in range(n):\n",
        "        \n",
        "        typhoon_features.append(np.load('./Testing Data-Joint Model/typhoon_features/typhoon_features'+str(i)+'.npy'))\n",
        "        labels.append(np.int_(np.load('./Testing Data-Joint Model/labels/labels'+str(i)+'.npy')))\n",
        "        \n",
        "        all_aggreTweets.append(np.int_(np.load('./Testing Data-Joint Model/all_aggreTweets/all_aggreTweets'+str(i)+'.npy')))\n",
        "        tweetsCounts.append(np.int_(np.load('./Testing Data-Joint Model/tweetsCounts/tweetsCounts'+str(i)+'.npy')))\n",
        "    \n",
        "    return typhoon_features,labels,all_aggreTweets,tweetsCounts\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3QDwnhXSdU1k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_features,training_labels,Tweets_train,tweetsCounts_train=load_training_Batches()\n",
        "testing_features,testing_labels,Tweets_test,tweetsCounts_test=load_testing_Batches()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cPsoU3VsfsAQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize Training and Testing in Tensorboard"
      ]
    },
    {
      "metadata": {
        "id": "E4tvIRXqfxuJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "\n",
        "tbc=TensorBoardColab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kin_X9sgZB6H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Joint-Model (LSTM+DNN) - Computation Graph"
      ]
    },
    {
      "metadata": {
        "id": "xO93_okHZD8t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "LSTM_DNN_Graph=tf.Graph()\n",
        "\n",
        "with LSTM_DNN_Graph.as_default():\n",
        "    \n",
        "    with tf.name_scope('LSTM_Placeholders') as scope:\n",
        "        tweets_inputs=tf.placeholder(tf.int32,[None,maxSeqLength],name='AllTweets_input')\n",
        "        senti_labels=tf.placeholder(tf.int32,[senti_batch_size,senti_classes],name='sentiment_labels')\n",
        "\n",
        "        tweets_Sizes=tf.placeholder(tf.int32,[typhoon_batchSize],name='AggreTweets_Sizes')\n",
        "    \n",
        "    with tf.name_scope('DNN_Placeholder') as scope:          \n",
        "        typhoon_X=tf.placeholder(name='Typhoon_Input',dtype=tf.float32,shape=[None,None]) # it can accept training one by one\n",
        "        typhoon_ytrue=tf.placeholder(name='Typhoon_Label',dtype=tf.float32,shape=[None,typhoon_classes])\n",
        "        \n",
        "    with tf.device('/gpu:0'):\n",
        "        with tf.name_scope('LSTM_Model') as scope:\n",
        "          \n",
        "            senti_predictions=LSTM_Model(tweets_inputs,transfer_learning=False)\n",
        "\n",
        "            sentiTweets_pred=senti_predictions[:senti_batch_size] \n",
        "            typhoonTweet_pred=senti_predictions[senti_batch_size:]\n",
        "\n",
        "            lstm_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=sentiTweets_pred,\n",
        "                                                                                        labels=senti_labels)) \n",
        "\n",
        "    with tf.name_scope('Tweets_Statistics') as scope:\n",
        "            # Tweets aggrgation and Analysis\n",
        "            def condition(tweets_Sizes,typhoonTweet_pred,typhoon_x,newTyphoon_x,j,i):            \n",
        "                return tf.less(i, tf.shape(tweets_Sizes)[0])\n",
        "\n",
        "            def body(tweets_Sizes,typhoonTweet_pred,typhoon_x,newTyphoon_x,j,i):\n",
        "\n",
        "                #get prediction of one typhoon at a time; here we do slicing to typhoonTweet_pred tensor\n",
        "                # i: index of aggreTweet size in Tweets_size tensor; while j step-by index in predictions tensor\n",
        "                typhoon_prediction=typhoonTweet_pred[j:tweets_Sizes[i]] \n",
        "\n",
        "                #tweetsCount=tf.cast(tf.shape(typhoon_prediction)[0],tf.float32)\n",
        "                tweetsCount=tf.cast(tf.nn.embedding_lookup(tweets_Sizes,i),tf.float32)\n",
        "\n",
        "                # get mean and variance\n",
        "                _, variance = tf.nn.moments(typhoonTweet_pred, [0])    \n",
        "                std=tf.sqrt(variance)\n",
        "                tweets_features=tf.concat([[tweetsCount],std],0)\n",
        "                tweets_features_Normalized=tf.nn.l2_normalize(tweets_features,dim=0)\n",
        "\n",
        "                # Get Current typhoon instance from Typhoon batch:\n",
        "                current_Typhoon=tf.nn.embedding_lookup(typhoon_x,i)\n",
        "                # extend tweets feature with typhoon     \n",
        "                new_Typhoon=tf.concat([current_Typhoon,tweets_features_Normalized],0)\n",
        "                newTyphoon_x=newTyphoon_x.write(i,new_Typhoon)\n",
        "\n",
        "                return tweets_Sizes,typhoonTweet_pred,typhoon_x,newTyphoon_x,tweets_Sizes[i],i+1\n",
        "\n",
        "            newTyphoon_x= tf.TensorArray(dtype=tf.float32,size=0,dynamic_size=True,element_shape=(featuresCount,))    \n",
        "            _,_,_,newTyphoon_x,_,_=tf.while_loop(condition, body, [tweets_Sizes,typhoonTweet_pred,typhoon_X,newTyphoon_x, 0,0])       \n",
        "            newTyphoon_x = newTyphoon_x.stack()\n",
        "    with tf.device('/gpu:0'):\n",
        "        with tf.name_scope('Typhoon_Model') as scope:                                                                                    \n",
        "            typhoon_ypred=DNN_model(newTyphoon_x)\n",
        "            \n",
        "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    \n",
        "            with tf.control_dependencies(update_ops):              \n",
        "              # Ensures that we execute the update_ops before performing the train_step                        \n",
        "              dnn_loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=typhoon_ypred,\n",
        "                                                                            labels=typhoon_ytrue))                                                                                                   \n",
        "              joint_Loss=dnn_loss+lstm_loss\n",
        "             \n",
        "             \n",
        "              \n",
        "              gradients = tf.gradients(joint_Loss, tf.trainable_variables())\n",
        "              clipper_, _ = tf.clip_by_global_norm(gradients, grad_clip_margin)\n",
        "            \n",
        "              optimizer = tf.train.AdamOptimizer(0.001)\n",
        "              joint_opt = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
        "            \n",
        "              predict_ = tf.equal(tf.argmax(typhoon_ypred,1), tf.argmax(typhoon_ytrue,1))\n",
        "\n",
        "              joint_accuracy=tf.reduce_mean(tf.cast(predict_, tf.float32))\n",
        "            \n",
        "              y_p=tf.argmax(typhoon_ypred,1)\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hRmSFbRjcUnQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Joint Model (LSTM+DNN) - Execution Session"
      ]
    },
    {
      "metadata": {
        "id": "0cQsHlG5cbDg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logdir1 = \"./Graph/LSTM+DNN(training)\"\n",
        "logdir2 = \"./Graph/LSTM+DNN(test)\"\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth=True\n",
        "config.allow_soft_placement=True\n",
        "config.log_device_placement=True\n",
        "\n",
        "epochs=100\n",
        "training_iteration=79 \n",
        "testing_iteration=19\n",
        "\n",
        "\n",
        "with tf.Session(graph=LSTM_DNN_Graph,config=config) as sess:\n",
        "    sess.run(tf.global_variables_initializer())   \n",
        "    \n",
        "    writer1 = tf.summary.FileWriter(logdir1, sess.graph)\n",
        "    writer2 = tf.summary.FileWriter(logdir2, sess.graph)\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "        \n",
        "    for j in range(epochs):        \n",
        "        \n",
        "        accuracy=[]\n",
        "                      \n",
        "        loss=[]\n",
        "        \n",
        "        for i in range(training_iteration):\n",
        "            \n",
        "            senti_x,senti_y=load_Sentiment_Training_Batch()            \n",
        "            all_tweets=np.concatenate((senti_x,Tweets_train[i]),axis=0)\n",
        "            \n",
        "            feed_dict={typhoon_X:training_features[i],typhoon_ytrue:training_labels[i],\n",
        "                       tweets_inputs:all_tweets,senti_labels:senti_y,tweets_Sizes:tweetsCounts_train[i]}\n",
        "\n",
        "            sess.run([joint_opt],feed_dict=feed_dict)\n",
        "            \n",
        "            loss_value,accuracy_value=sess.run([joint_Loss,joint_accuracy],feed_dict=feed_dict)\n",
        "                      \n",
        "            \n",
        "            accuracy.append(accuracy_value)\n",
        "            loss.append(loss_value)\n",
        "                                \n",
        "        value = tf.Summary.Value(tag=\"Loss\",simple_value=np.mean(loss))\n",
        "        summary = tf.Summary(value=[value])\n",
        "        writer1.add_summary(summary,j)\n",
        "          \n",
        "                                                                               \n",
        "        value = tf.Summary.Value(tag=\"Accuracy\",simple_value=np.mean(accuracy))\n",
        "        summary = tf.Summary(value=[value])\n",
        "        writer1.add_summary(summary,j)\n",
        "\n",
        "        # Evaluating testdata\n",
        "        accuracy=[]\n",
        "        loss=[]\n",
        "        precision=[]\n",
        "        recall=[]\n",
        "        f1=[]\n",
        "            \n",
        "        for i in range(testing_iteration):\n",
        "                    \n",
        "          senti_x,senti_y=load_Sentiment_Testing_Batch()\n",
        "            \n",
        "          all_tweets=np.concatenate((senti_x,Tweets_test[i]),axis=0)\n",
        "          \n",
        "          feed_dict={typhoon_X:testing_features[i],typhoon_ytrue:testing_labels[i],\n",
        "                   tweets_inputs:all_tweets,senti_labels:senti_y,tweets_Sizes:tweetsCounts_test[i]}\n",
        "          \n",
        "          loss_value,accuracy_value,typhoon_ypred_value=sess.run([joint_Loss,joint_accuracy,y_p],feed_dict=feed_dict)\n",
        "          \n",
        "          accuracy.append(accuracy_value)\n",
        "          loss.append(loss_value)\n",
        "          \n",
        "     \n",
        "          y_true = np.argmax(testing_labels[i],1)\n",
        "          \n",
        "          precision.append(precision_score(y_true, typhoon_ypred_value,average='weighted'))\n",
        "          recall.append(recall_score(y_true, typhoon_ypred_value,average='weighted'))\n",
        "          f1.append(f1_score(y_true, typhoon_ypred_value,average='weighted'))\n",
        "\n",
        "          \n",
        "        value = tf.Summary.Value(tag=\"Loss\",simple_value=np.mean(loss))\n",
        "        summary = tf.Summary(value=[value])\n",
        "        writer2.add_summary(summary,j)\n",
        "                    \n",
        "        value = tf.Summary.Value(tag=\"Accuracy\",simple_value=np.mean(accuracy))\n",
        "        summary2 = tf.Summary(value=[value])\n",
        "        writer2.add_summary(summary2,j)\n",
        "        \n",
        "        \n",
        "    print (\"Performance Results (Accuracy, Precision, Recall and F1-Score):\",np.mean(accuracy),np.mean(precision),np.mean(recall),np.mean(f1))                     \n",
        "    \n",
        "    writer1.close() \n",
        "    writer2.close()     "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}