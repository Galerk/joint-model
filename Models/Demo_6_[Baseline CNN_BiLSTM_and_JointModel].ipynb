{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo_6_JointModel(BiLSTM+CNN).ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "r8evTSb3vwL9",
        "eul9z6ZDnRKv",
        "_ZbInHteEOme",
        "fBmst626VtKP",
        "5ugczT4oPYqo"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS-m0Cg8jsmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input,Dropout, Dense, Embedding, TimeDistributed, Bidirectional, GlobalMaxPooling1D, Conv1D, Flatten, MaxPooling1D, LSTM\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Reshape\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import f1_score,precision_score, recall_score, accuracy_score\n",
        "\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec \n",
        "\n",
        "from nltk.tokenize import TweetTokenizer \n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "from random import randint\n",
        "import random\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk1-yMkoKkJ-",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing Steps:\n",
        "\n",
        "\n",
        "1.   Load Dataset (typhoon tweets, sentiment140)\n",
        "2.   Build Word Embedding Model or Load Pre-trained Model\n",
        "3.   Create Feature Extractor Model (Bi-LSTM)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY0Txgb-M3qb",
        "colab_type": "text"
      },
      "source": [
        "## Helper Functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKC1_h3WL3aM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading sentiment140 dataset and project (tweets text,label). It also maps labels (4:1) in the original dataset.\n",
        "def ingest():\n",
        "    data = pd.read_csv('./tweets.csv', encoding = \"ISO-8859-1\") \n",
        "    data.drop(['ItemID', 'SentimentSource'], axis=1, inplace=True)\n",
        "    data = data[data.Sentiment.isnull() == False]\n",
        "    data['Sentiment'] = data['Sentiment'].map( {4:1,0:0})\n",
        "    data = data[data['SentimentText'].isnull() == False]\n",
        "    data.reset_index(inplace=True)\n",
        "    data.drop('index', axis=1, inplace=True)    \n",
        "    data=shuffle(data) #randmoize sequence of data\n",
        "    print(('dataset loaded with shape', data.shape))    \n",
        "\n",
        "    return data\n",
        "\n",
        "# extract tweets text and label; also it maps label (4) to (1) as a positive \n",
        "def postprocess(data, n=1600000): # loading 1.6 million tweets\n",
        "    data = data.head(n)\n",
        "    data['tokens'] = data['SentimentText'].apply(tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n",
        "    data = data[data.tokens != 'NC']\n",
        "    data.reset_index(inplace=True)\n",
        "    data.drop('index', inplace=True, axis=1)\n",
        "    return data\n",
        "\n",
        "#tokenizing tweets: clean hashtags,usernames, and stop words. return list of words\n",
        "def tokenize(tweet):\n",
        "    try:\n",
        "        tokens = tokenizer.tokenize(tweet.lower())\n",
        "        tokens = list([t for t in tokens if not t.startswith('@')])\n",
        "        tokens = list([t for t in tokens if not t.startswith('#')])\n",
        "        tokens = list([t for t in tokens if not t.startswith('http')])\n",
        "        tokens = list([t for t in tokens if not t.startswith('https')])\n",
        "        \n",
        "        #keep only text tweets, ignore numbers\n",
        "        tokens=list([t for t in tokens if t.isalpha()])\n",
        "        \n",
        "        return tokens\n",
        "    except:\n",
        "        return 'NC'\n",
        "\n",
        "# build a labeledSentence from tweet's text to train the word embedding model.\n",
        "def labelizeTweets(tweets, label_type):\n",
        "    labelized = []\n",
        "    for i,v in enumerate(tweets):\n",
        "        label = '%s_%s'%(label_type,i)\n",
        "        labelized.append(TaggedDocument(v, [label]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G6T0RcxM_GL",
        "colab_type": "text"
      },
      "source": [
        "## Load Sentiment140 Dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThdkOnQkM_Tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment140=ingest() # loading 1.6 Million Labelled tweets\n",
        "sentiment140=postprocess(sentiment140) # clearning and representing data as tweet and sentiment\n",
        "\n",
        "# separate Training Features (X) from Labels (Y)\n",
        "n=1600000 # ......data size 1.6 million tweets..\n",
        "x_sentiment,y_sentiment = np.array(sentiment140.head(n).tokens),np.array(sentiment140.head(n).Sentiment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IZWHfz_N7Aa",
        "colab_type": "text"
      },
      "source": [
        "## Load Typhoon Tweets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18Xx0OutN-Tz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "typhoon_df=pd.DataFrame()\n",
        "path ='./TED Dataset/Typhoons_tweets' # Use your path\n",
        "allFiles = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "# merge all typhoon tweets into one file\n",
        "list_ = []\n",
        "for file_ in allFiles:\n",
        "    df = pd.read_csv(file_,index_col=None, header=0)\n",
        "    list_.append(df) # append all tweets_lists into one list\n",
        "    \n",
        "typhoon_df = pd.concat(list_) # merage all tweets together.\n",
        "typhoon_tweets=typhoon_df['text'].tolist()\n",
        "\n",
        "tweetsTokens=list()\n",
        "\n",
        "#Tweets preprocessing\n",
        "for tweet in tqdm(typhoon_tweets):\n",
        "    tweet=str(tweet)\n",
        "    tweetsTokens.append(tokenize(tweet)) # clean tweets and append to tweets_list\n",
        "\n",
        "typhoon_tweets_tokens=tweetsTokens # tokenized tweets."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfOo_JeROEcn",
        "colab_type": "text"
      },
      "source": [
        "## Build Word Embedding Model:\n",
        "\n",
        "**If : you would to train a word embedding model -from scratch- RUN the first cell ; Else: RUN the 2nd cell to load a pre-trained model directly; **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5lpfU30OHQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------- Build A Word Embedding Model (Word2vec) ----------------------- #\n",
        "\n",
        "# combine all twees together. \n",
        "allTweets=sentiment140['tokens'].tolist()+typhoon_tweets_tokens\n",
        "\n",
        "# building all word embedding (Words)\n",
        "allTweets = labelizeTweets(allTweets, 'AllTWEETS')\n",
        "\n",
        "EMBEDDING_DIM=200 # word2vec dimension\n",
        "\n",
        "word_emb = Word2Vec(size=n_dim, min_count=10,sg=1) #sg=1 Skipgram is used\n",
        "word_emb.build_vocab([x.words for x in tqdm(allTweets)]) # words attribute by LabeledSentence\n",
        "word_emb.train([x.words for x in tqdm(allTweets)],total_examples=len(allTweets),epochs=10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTW2qB4DOYxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ----------------------- Load a Pre-trained Word Embedding Model -----------------#\n",
        "\n",
        "EMBEDDING_DIM=200\n",
        "# Load pre-trained word embedding model\n",
        "word_emb=Word2Vec.load('./wordEmbedding.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqn_9ryNOc13",
        "colab_type": "text"
      },
      "source": [
        "## Enriching Word Embedding Model with Semantics Vectors from ConceptNet KG:\n",
        "* Extract Entities Using Spacy\n",
        "* Get Entities Vectors From ConceptNet\n",
        "* Build Features Matrix Of Feature Extractor Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4t0skBSPJpw",
        "colab_type": "text"
      },
      "source": [
        "### Extract Entities Using Spacy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eqTxVGFOkji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extract Entities From Tweets Using Spacy\n",
        "\n",
        "tweets_entities=set() # list of recoginized entities\n",
        "\n",
        "for tweet in tqdm(typhoon_tweets):\n",
        "  \n",
        "  entities=nlp(str(tweet))\n",
        "  \n",
        "  for ent in entities.ents:\n",
        "    tweets_entities.add(ent.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXy7EEoxPNsK",
        "colab_type": "text"
      },
      "source": [
        "### Get Entities Vectors From ConceptNet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48MXn99BO0b2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Semantic Vectors From Knowledge Graph (ConceptNet)\n",
        "\n",
        "# Load ConceptNet Embedding Model\n",
        "conceptnet_model=gensim.models.KeyedVectors.load_word2vec_format('./numberbatch-en-17.06.txt.gz') # use your own path.\n",
        "\n",
        "# Get list of words from ConceptNet Embedding Model\n",
        "conceptnet_model_words=list(conceptnet_model.wv.vocab)\n",
        "# Get corresponding semantic vectors of words\n",
        "conceptnet_model_wordVectors=conceptnet_model[conceptnet_model.wv.vocab]\n",
        "\n",
        "conceptnet_vectors={}\n",
        "\n",
        "for w , vec in tqdm(zip(conceptnet_model_words,conceptnet_model_wordVectors)):\n",
        "  conceptnet_vectors[w]=vec\n",
        "  \n",
        "  \n",
        "# Loading word embeddiing vocabs\n",
        "tweets_model_words=list(tweets_model.wv.vocab)\n",
        "tweets_model_wordVectors=tweets_model[tweets_model.wv.vocab]\n",
        "\n",
        "# maintain a dictionary to keep words with their corresponding vectors. \n",
        "# Entities are represented with their semantic vectors from ConceptNet Knowledage Graph\n",
        "\n",
        "word_vectors_dict={}\n",
        "\n",
        "# iterate over words list and vectors list and save into dict:\n",
        "for word, vector in tqdm(zip(tweets_model_words,tweets_model_wordVectors)):\n",
        "  \n",
        "  # check if word is an entity and if word has a semantic vector\n",
        "  if word in tweets_entities and word in conceptnet_vectors:     \n",
        "      word_vectors_dict[word]=conceptnet_vectors[word]\n",
        "      \n",
        "  else: # otherwise get its word2vec\n",
        "      word_vectors_dict[w]=vector\n",
        "      \n",
        "#------------ Buidling words indices ----------#\n",
        "words_indices={} # from tweets2vec model\n",
        "\n",
        "i=0\n",
        "for w in list(word_vectors_dict.keys()):\n",
        "  words_indices[w]=i\n",
        "  i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEOeWHvLPSza",
        "colab_type": "text"
      },
      "source": [
        "### Build Features Matrix Of Feature Extractor Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUis9bV9PEAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxSeqLength = 20# based on average count of words per tweets in training dataset\n",
        "\n",
        "sentiment_features = np.zeros((len(x_sentiment), maxSeqLength), dtype=int)\n",
        "\n",
        "for instance in tqdm(x_sentiment):    \n",
        "    vectors=np.zeros(maxSeqLength,dtype=int)\n",
        "   \n",
        "    j=0  # word counter\n",
        "    for word in instance[:maxSeqLength]:\n",
        "        if word in word_vectors_dict:\n",
        "            vectors[j]=words_indices[word] # get word index\n",
        "\n",
        "        j+=1\n",
        "    \n",
        "    sentiment_features[i]=vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwvRG_FgPW3f",
        "colab_type": "text"
      },
      "source": [
        "## Encode tweets labels into one-hot vectors:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ie7Rk7wPXPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# representing class data as one-hot vectors\n",
        "y_sentiment_ = np.array([y_sentiment]).reshape(-1)\n",
        "one_hot_targets = np.eye(2)[y_sentiment_]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUHFwNl2Pigt",
        "colab_type": "text"
      },
      "source": [
        "## Split data into train-test sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGWqJORkPla7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split data to train and test\n",
        "split_frac = 0.8\n",
        "split_idx = int(len(sentiment_features)*split_frac)\n",
        "\n",
        "senti_train_x, senti_val_x = sentiment_features[:split_idx], sentiment_features[split_idx:]\n",
        "senti_train_y, senti_val_y = one_hot_targets[:split_idx], one_hot_targets[split_idx:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omTp246OglKT",
        "colab_type": "text"
      },
      "source": [
        "# Baseline Models:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khDuCQdzgqVt",
        "colab_type": "text"
      },
      "source": [
        "## Load TED dataset for the baseline models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VI---5igxZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TED_dataset=np.load('TED.npy', allow_pickle=True)\n",
        "\n",
        "Typhoon_Env=TED_dataset[:,0]\n",
        "Typhoon_labels=[]\n",
        "Typhoon_features=[]\n",
        "\n",
        "for i in range (len(Typhoon_Env)):\n",
        "  Typhoon_labels.append(Typhoon_Env[i][0])\n",
        "  Typhoon_features.append(Typhoon_Env[i][1:-1])\n",
        "\n",
        "# Encode typhoon labels into one-hot\n",
        "label_encoder=LabelEncoder()\n",
        "Typhoon_labels_encoded=label_encoder.fit_transform(Typhoon_labels)\n",
        "\n",
        "Typhoon_labels_encoded = to_categorical(Typhoon_labels_encoded,dtype='int')\n",
        "\n",
        "Typhoon_features=np.asarray(Typhoon_features)\n",
        "Typhoon_labels_encoded=np.asarray(Typhoon_labels_encoded)\n",
        "\n",
        "# split the dataset into train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(Typhoon_features,Typhoon_labels_encoded, test_size=0.20, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZbInHteEOme",
        "colab_type": "text"
      },
      "source": [
        "## The Baseline Model (CNN):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-lUm3g8Fq8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create model\n",
        "CNNmodel = Sequential()\n",
        "\n",
        "#add model layers\n",
        "CNNmodel.add(Conv1D(32, kernel_size=3, activation='relu'))\n",
        "CNNmodel.add(Dropout(0.3))\n",
        "CNNmodel.add(Conv1D(16, kernel_size=3, activation='relu'))\n",
        "CNNmodel.add(Dropout(0.2))\n",
        "CNNmodel.add(MaxPooling1D(pool_size=(8)))\n",
        "CNNmodel.add(Flatten())\n",
        "CNNmodel.add(Dense(4, activation='softmax'))\n",
        "\n",
        "CNNmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBmst626VtKP",
        "colab_type": "text"
      },
      "source": [
        "### Traing and evaluate the CNN Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt-NFyYNV8tO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## train the CNN model ##\n",
        "X_train_reshaped=X_train.reshape(len(X_train),12,1,1)\n",
        "CNNmodel.fit(X_train_reshaped, y_train, epochs=30,batch_size=64)\n",
        "\n",
        "## test the CNN model ##\n",
        "X_test_reshaped=X_test.reshape(-1,12,1)\n",
        "eval_model=CNNmodel.evaluate(X_train_reshaped, y_train)\n",
        "\n",
        "y_pred=CNNmodel.predict(X_test_reshaped)\n",
        "y_pred_binarized =(y_pred>0.5)\n",
        "print ('F1-score ',f1_score(y_true=y_test, y_pred=y_pred_binarized, average='micro'))\n",
        "print ('Precision score', precision_score(y_true=y_test,y_pred=y_pred_binarized,average='micro'))\n",
        "print ('Recall score', recall_score(y_true=y_test,y_pred=y_pred_binarized,average='micro'))\n",
        "print ('Accuracy score', accuracy_score(y_true=y_test, y_pred=y_pred_binarized))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tHoQtKtLaC-",
        "colab_type": "text"
      },
      "source": [
        "## The Baseline Model (BiLSTM):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA9BJoLEL8N7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BiLSTM_model = Sequential()\n",
        "n_timesteps=10\n",
        "\n",
        "BiLSTM_model.add(Bidirectional(LSTM(64, return_sequences=True),input_shape=(12,1)))\n",
        "BiLSTM_model.add(GlobalMaxPooling1D())\n",
        "BiLSTM_model.add(Dense(32, activation=\"relu\"))\n",
        "BiLSTM_model.add(Dropout(0.25))\n",
        "BiLSTM_model.add(Dense(4, activation=\"softmax\"))\n",
        "\n",
        "BiLSTM_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ugczT4oPYqo",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate the Bi-LSTM Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT5sOjSoPjsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## train the BiLSTM model ##\n",
        "X_train_reshaped=X_train.reshape(-1,12,1)\n",
        "\n",
        "BiLSTM_model.fit(X_train_reshaped,y_train,epochs=50,batch_size=64)\n",
        "\n",
        "## test the BiLSTM model ##\n",
        "X_test_reshaped=X_test.reshape(-1,12,1)\n",
        "eval_model=BiLSTM_model.evaluate(X_train_reshaped, y_train)\n",
        "y_pred=BiLSTM_model.predict(X_test_reshaped)\n",
        "\n",
        "y_pred_binarized =(y_pred>0.5)\n",
        "\n",
        "print ('F1-score ',f1_score(y_true=y_test, y_pred=y_pred_binarized, average='micro'))\n",
        "print ('Precision score', precision_score(y_true=y_test,y_pred=y_pred_binarized,average='micro'))\n",
        "print ('Recall score', recall_score(y_true=y_test,y_pred=y_pred_binarized,average='micro'))\n",
        "print ('Accuracy score', accuracy_score(y_true=y_test, y_pred=y_pred_binarized))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VakGG0WKpO_H",
        "colab_type": "text"
      },
      "source": [
        "# The JointModel (BiLSTM+CNN):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96UuRu2mUZKI",
        "colab_type": "text"
      },
      "source": [
        "## Load TED dataset (typhoon environmental data and tweets)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JLHwsWncRJlk",
        "colab": {}
      },
      "source": [
        "TED_dataset=np.load('TED.npy', allow_pickle=True)\n",
        "\n",
        "# split the dataset into typhoon environmental data and typhoon tweets\n",
        "Typhoon_Env=TED_dataset[:,0]\n",
        "Typhoon_tweets=TED_dataset[:,1]\n",
        "\n",
        "Typhoon_groundTruth=[]\n",
        "Typhoon_features=[]\n",
        "\n",
        "for i in range(len(Typhoon_Env)):\n",
        "  Typhoon_groundTruth.append(Typhoon_Env[i][0])\n",
        "\n",
        "  # remove typhoon label (at index 0) and timstamp (at last index) from typhoon features and add tweets count\n",
        "  feat_vect=Typhoon_Env[i][1:-1]+[len(Typhoon_tweets[i])]\n",
        "  Typhoon_features.append(feat_vect)\n",
        "\n",
        "Typhoon_features=np.array(Typhoon_features,dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCvzPyYn0BhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Typhoon_tweets=np.zeros((len(Typhoon_tweets),100, maxSeqLength), dtype=int)\n",
        "\n",
        "i=0\n",
        "for tweet in Typhoon_tweets1:\n",
        "  typhoon_twt=np.array(tweet)\n",
        "  typhoon_twt.resize(100,maxSeqLength)\n",
        "\n",
        "  Typhoon_tweets4[i]=typhoon_twt\n",
        "  i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmXUCwY9bo8i",
        "colab_type": "text"
      },
      "source": [
        "### Encode Typhoon Classes into One-hot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJARXxi__fHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_encoder=LabelEncoder()\n",
        "Typhoon_labels_encoded=label_encoder.fit_transform(Typhoon_groundTruth)\n",
        "\n",
        "Typhoon_labels_encoded = to_categorical(Typhoon_labels_encoded,dtype='int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A40RI0HvbudJ",
        "colab_type": "text"
      },
      "source": [
        "### Split TED data into train, test splits:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIoMxV9yVwtK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_frac = 0.8\n",
        "split_idx = int(len(Typhoon_features)*split_frac)\n",
        "\n",
        "Typhoon_train_x, Typhoon_val_x = Typhoon_features[:split_idx], Typhoon_features[split_idx:]\n",
        "TyphoonLabels_train_y, TyphoonLabels_val_y = Typhoon_labels_encoded[:split_idx], Typhoon_labels_encoded[split_idx:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBap_lXpCZe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Typhoon_tweets_x, Typhoon_tweets_y = Typhoon_tweets[:split_idx], Typhoon_tweets[split_idx:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dffjGtcoOzFq",
        "colab_type": "text"
      },
      "source": [
        "## The JointModel (BiLSTM+CNN):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfgMSRDkXOxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load pretrained weights\n",
        "featExtractor_model=create_featExtractor_Model()\n",
        "featExtractor_model.load_weights('./feat_Extractor_weights_.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5uRMOegybjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_jointModel():\n",
        "  # load pretrained weights\n",
        "  featExtractor_model=create_featExtractor_Model()\n",
        "  featExtractor_model.load_weights('./feat_Extractor_weights_.h5')\n",
        "\n",
        "  typhoon_env=Input(shape=(13,)) # 13 : features of typhoon environmental data\n",
        "  concatenated_feat=concatenate([featExtractor_model.output,typhoon_env], axis=-1)\n",
        "\n",
        "  concatenated_feat=Reshape((15,1))(concatenated_feat) # 15: final dimension of 13 (env)+2 (feature_extractorModel)\n",
        "  \n",
        "  typhoon_Conv1D1=Conv1D(32, kernel_size=3, activation='relu')(concatenated_feat)\n",
        "  dropout_layer1=Dropout(0.3)(typhoon_Conv1D1)\n",
        "\n",
        "  typhoon_Conv1D2=Conv1D(16, kernel_size=3, activation='relu')(dropout_layer1)\n",
        "  dropout_layer2=Dropout(0.2)(typhoon_Conv1D2)\n",
        "\n",
        "  maxPooling1D_layer=MaxPooling1D(pool_size=(8))(dropout_layer2)\n",
        "  flatted_layer=Flatten()(maxPooling1D_layer)\n",
        "  typhoon_predictions=Dense(4, activation='softmax')(flatted_layer)\n",
        "\n",
        "  jointModel=Model(inputs=typhoon_input,outputs=[typhoon_predictions,featExtractor_model.output])\n",
        "\n",
        "  featExtractor_model.trainable=True # set the feature_Extractor model trainable with the loss in typhoon_Classifier Model\n",
        "\n",
        "  jointModel.compile(optimizer='adam', loss=['categorical_crossentropy','categorical_crossentropy'], loss_weights=[1,1] , metrics=['accuracy'])\n",
        "\n",
        "  return jointModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVaz9UqTfeHM",
        "colab_type": "text"
      },
      "source": [
        "### Train and evaluate the JointModel:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgL_CBawyf8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jointModel=create_jointModel()\n",
        "## train the jointModel (BiLSTM+CNN) ##\n",
        "jointModel.fit([X_train_reshaped,X_train], [y_train,y_train], epochs=30,batch_size=64)\n",
        "\n",
        "## test the jointModel ##\n",
        "y_pred=typhoon_Classifer.predict([Typhoon_tweets1_y,Typhoon_val_x])\n",
        "y_pred_binarized =(y_pred>0.5)\n",
        "\n",
        "print ('F1-score ',f1_score(y_true=TyphoonLabels_val_y, y_pred=y_pred_binarized, average='micro'))\n",
        "print ('Precision score', precision_score(y_true=TyphoonLabels_val_y,y_pred=y_pred_binarized,average='micro'))\n",
        "print ('Recall score', recall_score(y_true=TyphoonLabels_val_y,y_pred=y_pred_binarized,average='micro'))\n",
        "print ('Accuracy score', accuracy_score(y_true=TyphoonLabels_val_y, y_pred=y_pred_binarized))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}