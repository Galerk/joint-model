{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo1 [Baseline RNN and Joint Model (LSTM+RNN) ]",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "crORy8tYy_5M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Set Up"
      ]
    },
    {
      "metadata": {
        "id": "XHRZZv3YzUmv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install numpy pandas gensim tqdm nltk pandas_ml imblearn tensorboardcolab\n",
        "!pip3 install scikit-learn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U9TDcqluMAn0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "pd.options.mode.chained_assignment = None\n",
        "import numpy as np \n",
        "from copy import deepcopy\n",
        "from string import punctuation\n",
        "\n",
        "import glob\n",
        "import json\n",
        "\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec \n",
        "TaggedDocument = gensim.models.doc2vec.TaggedDocument \n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer \n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from random import randint\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import random\n",
        "import pandas_ml as pdml\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "he96WMzfLujH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Preprocessing Steps:\n",
        "\n",
        "\n",
        "1.  Load Dataset (typhoon tweets, sentiment140)\n",
        "2.  Build Word Embedding Model or Load Pre-trained Model\n",
        "3.  Create Sentiment Features for Feature Extractor Model (LSTM)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "oRb6-vr-NFJP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Helper functions for preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "FWYTaR9KNJ-5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#loading sentiment140 training dataset,and project only (tweets text,class (0 or 1)). It also maps labels (4:1) in the original dataset.\n",
        "def ingest():\n",
        "    data = pd.read_csv('./Sentiment140/training.1600000.processed.noemoticon.csv', encoding = \"ISO-8859-1\") # use your path\n",
        "\n",
        "    data.drop(['ItemID', 'SentimentSource'], axis=1, inplace=True)\n",
        "    data = data[data.Sentiment.isnull() == False]\n",
        "    data['Sentiment'] = data['Sentiment'].map( {4:1,0:0})\n",
        "    data = data[data['SentimentText'].isnull() == False]\n",
        "    data.reset_index(inplace=True)\n",
        "    data.drop('index', axis=1, inplace=True)    \n",
        "    data=shuffle(data) #randmoize sequence of data\n",
        "    print(('dataset loaded with shape', data.shape))    \n",
        "\n",
        "    return data\n",
        "\n",
        "# extract tweets text and label; also it maps label (4) to (1) as a positive \n",
        "def postprocess(data, n=1600000): # loading 1.6 million tweets\n",
        "    data = data.head(n)\n",
        "    data['tokens'] = data['SentimentText'].progress_map(tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n",
        "    data = data[data.tokens != 'NC']\n",
        "    data.reset_index(inplace=True)\n",
        "    data.drop('index', inplace=True, axis=1)\n",
        "    return data\n",
        "\n",
        "#tokenizing tweets: clean hashtags,usernames, and stop words. return list of words\n",
        "def tokenize(tweet):\n",
        "    try:\n",
        "        tokens = tokenizer.tokenize(tweet.lower())\n",
        "        tokens = list([t for t in tokens if not t.startswith('@')])\n",
        "        tokens = list([t for t in tokens if not t.startswith('#')])\n",
        "        tokens = list([t for t in tokens if not t.startswith('http')])\n",
        "        tokens = list([t for t in tokens if not t.startswith('https')])\n",
        "        \n",
        "        #keep only text tweets, ignore numbers\n",
        "        tokens=list([t for t in tokens if t.isalpha()])\n",
        "        \n",
        "        return tokens\n",
        "    except:\n",
        "        return 'NC'\n",
        "\n",
        "# build a labeledSentence from tweet's text to train the word embedding model.\n",
        "def labelizeTweets(tweets, label_type):\n",
        "    labelized = []\n",
        "    for i,v in tqdm(enumerate(tweets)):\n",
        "        label = '%s_%s'%(label_type,i)\n",
        "        labelized.append(TaggedDocument(v, [label]))\n",
        "    return labelized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f6dWr8mV64HM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 Load Sentiment140 Dataset"
      ]
    },
    {
      "metadata": {
        "id": "SktQ15pd62hc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentiment140=ingest() # loading 1.6 Million Labelled tweets\n",
        "sentiment140=postprocess(sentiment140) # clearning and representing data as tweet and sentiment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uBjdduM07MB9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# separate Training Features (X) from Labels (Y)\n",
        "\n",
        "n=1600000 # data size 1.6 million tweets\n",
        "x_sentiment,y_sentiment = np.array(sentiment140.head(n).tokens),np.array(sentiment140.head(n).Sentiment) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cnu75Ekq9NcJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 Load Typhoon Tweets"
      ]
    },
    {
      "metadata": {
        "id": "eErYwXiL9RNZ",
        "colab_type": "code",
        "outputId": "5638847b-a4e1-4c02-a60c-217e7ea5ca82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "typhoon_df=pd.DataFrame()\n",
        "path ='./TED Dataset/Typhoons_tweets' # We provided tweets ids, please download full tweets data into Typhoons_tweets folder\n",
        "allFiles = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "# merge all typhoon tweets into one file\n",
        "list_ = []\n",
        "for file_ in allFiles:\n",
        "    df = pd.read_csv(file_,index_col=None, header=0)\n",
        "    list_.append(df) # append all tweets_lists into one list\n",
        "    \n",
        "typhoon_df = pd.concat(list_) # merage all tweets together.\n",
        "typhoon_tweets=typhoon_df['text'].tolist()\n",
        "\n",
        "## tokenize tweets as a list of words ## \n",
        "tweetsTokens=list()\n",
        "\n",
        "#Tweets preprocessing\n",
        "for tweet in tqdm(typhoon_tweets):\n",
        "    tweet=str(tweet)\n",
        "    tweetsTokens.append(tokenize(tweet)) # clean tweets and append to tweets_list\n",
        "\n",
        "typhoon_tweets_tokens=tweetsTokens # tokenized tweets."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 678982/678982 [00:59<00:00, 11422.47it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "8RtdnXh47_xG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Build a Word Embedding Model.\n",
        "To train a word embedding model  -from scratch- RUN the first cell.\n",
        "\n",
        "\n",
        "OR\n",
        "\n",
        "Load a pre-trained word embedding model.\n"
      ]
    },
    {
      "metadata": {
        "id": "d60XsZzE8FJF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ---------------------------- Build A Word Embedding Model (Word2vec) ----------------------- #\n",
        "\n",
        "# combine all twees together. \n",
        "allTweets=sentiment140['tokens'].tolist()+typhoon_tweets_tokens\n",
        "\n",
        "# building all word embedding (Words)\n",
        "allTweets = labelizeTweets(allTweets, 'AllTWEETS')\n",
        "\n",
        "n_dim=200 # word2vec dimension\n",
        "\n",
        "word_emb = Word2Vec(size=n_dim, min_count=10,sg=1) #sg=1 Skipgram is used\n",
        "word_emb.build_vocab([x.words for x in tqdm(allTweets)]) # words attribute by LabeledSentence\n",
        "word_emb.train([x.words for x in tqdm(allTweets)],total_examples=len(allTweets),epochs=10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CcJxiZqh8Fbl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ----------------------- Load a Pre-trained Word Embedding Model -----------------#\n",
        "word_emb=Word2Vec.load('./wordEmbedding.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "diqE5rIOAa66",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train and Evaluate Baseline (RNN)"
      ]
    },
    {
      "metadata": {
        "id": "jaCOivRuL82P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Pre-processed Training and Test dataset"
      ]
    },
    {
      "metadata": {
        "id": "Mem-aVLsBzbb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# n: number of batches in training dataset. \n",
        "# In our preprocessed data, each batch has 32 typhoon sample. Total batches is 79\n",
        "\n",
        "def load_training_Batches(n=79): \n",
        "    typhoon_features=[]\n",
        "    labels=[]\n",
        "            \n",
        "    for i in range(n):\n",
        "        \n",
        "        typhoon_features.append(np.load('./Training Data-Baseline/typhoon_features/typhoon_features'+str(i)+'.npy'))\n",
        "        labels.append(np.int_(np.load('./Training Data-Baseline/labels/labels'+str(i)+'.npy')))\n",
        "\n",
        "    return typhoon_features,labels\n",
        "  \n",
        "# In testing data, there is 19 batches of testing data. Each batch has 32 typhoon sample  \n",
        "def load_testing_Batches(n=19): \n",
        "    typhoon_features=[]\n",
        "    labels=[]\n",
        "\n",
        "    for i in range(n):\n",
        "        \n",
        "        typhoon_features.append(np.load('./Testing Data-Baseline/typhoon_features/typhoon_features'+str(i)+'.npy'))\n",
        "        labels.append(np.int_(np.load('./Testing Data-Baseline/labels/labels'+str(i)+'.npy')))\n",
        "            \n",
        "    return typhoon_features,labels\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hKpzLTn5R8lh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_features,training_labels=load_training_Batches()\n",
        "testing_features,testing_labels=load_testing_Batches()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oI4jDlKGDhiK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## RNN Baseline Model"
      ]
    },
    {
      "metadata": {
        "id": "NNJZXltlDlD6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RNN Model Architecture"
      ]
    },
    {
      "metadata": {
        "id": "e2SjymrKDnIw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "typhoon_batchSize=32\n",
        "featuresCount=12\n",
        "typhoon_classes=4\n",
        "\n",
        "learningrate=0.001\n",
        "hidden_nodes=16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ddwva-hsS17h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def LSTM_cell(hidden_layer_size, batch_size,number_of_layers, dropout=True, dropout_rate=0.6):\n",
        "    \n",
        "    layer = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size)\n",
        "        \n",
        "    if dropout:\n",
        "        layer = tf.contrib.rnn.DropoutWrapper(layer, output_keep_prob=dropout_rate)\n",
        "        \n",
        "    cell = tf.contrib.rnn.MultiRNNCell([layer]*number_of_layers)\n",
        "    \n",
        "    init_state = cell.zero_state(batch_size, tf.float32)\n",
        "    \n",
        "    return cell, init_state\n",
        "\n",
        "\n",
        "def output_layer(lstm_output, in_size, out_size):\n",
        "    x = lstm_output[:, -1, :]\n",
        "    weights = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.05), name='output_layer_weights')\n",
        "    bias = tf.Variable(tf.zeros([out_size]), name='output_layer_bias')\n",
        "    \n",
        "    output = tf.matmul(x, weights) + bias\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kpYYUJWWS-tL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_layer_size=64\n",
        "learning_rate=0.001\n",
        "number_of_layers=1\n",
        "dropout=True\n",
        "dropout_rate=0.75\n",
        "grad_clip_margin=4\n",
        "\n",
        "def RNN_model(X_data): \n",
        "    \n",
        "    X_data=tf.reshape(X_data,[typhoon_batchSize,featuresCount,1])\n",
        "    \n",
        "    cell, _ = LSTM_cell(hidden_layer_size, typhoon_batchSize, number_of_layers, dropout, dropout_rate)\n",
        "    \n",
        "    outputs, states = tf.nn.dynamic_rnn(cell, X_data, dtype=tf.float32)\n",
        "    \n",
        "    logits=output_layer(outputs, hidden_layer_size, typhoon_classes)\n",
        "    \n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rlhdq4G0TGq6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Computation Graph"
      ]
    },
    {
      "metadata": {
        "id": "1sBeNrd3TJ0z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "RNN_graph=tf.Graph()\n",
        "\n",
        "with RNN_graph.as_default():\n",
        "    \n",
        "    #prameters for Typhoon Model\n",
        "    typhoon_X=tf.placeholder(name='Typhoon_Input',dtype=tf.float32,shape=[typhoon_batchSize,featuresCount])\n",
        "    typhoon_ytrue=tf.placeholder(name='Typhoon_Label',dtype=tf.float32,shape=[typhoon_batchSize,typhoon_classes])\n",
        "    \n",
        "    with tf.name_scope(\"RNN_Model\") as scope:                      \n",
        "        \n",
        "        typhoon_ypred=RNN_model(typhoon_X) # prediction with typhoon features only       \n",
        "        \n",
        "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "        with tf.control_dependencies(update_ops):\n",
        "            \n",
        "            # Ensures that we execute the update_ops before performing the train_step\n",
        "            rnn_loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "                logits=typhoon_ypred,labels=typhoon_ytrue))\n",
        "                                           \n",
        "            y_p=tf.argmax(typhoon_ypred,1)\n",
        "\n",
        "        \n",
        "            #Cliping the gradient loss\n",
        "            gradients = tf.gradients(rnn_loss, tf.trainable_variables())\n",
        "            clipper_, _ = tf.clip_by_global_norm(gradients, grad_clip_margin)\n",
        "            \n",
        "            optimizer = tf.train.AdamOptimizer()\n",
        "            rnn_opt = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
        "            \n",
        "            predict_ = tf.equal(tf.argmax(typhoon_ypred,1), tf.argmax(typhoon_ytrue,1))\n",
        "            rnn_accuracy = tf.reduce_mean(tf.cast(predict_, tf.float32))\n",
        "            \n",
        "            y_p=tf.argmax(typhoon_ypred,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JBapV7XCTec7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualizing Model Training and Testing in Tensorboard"
      ]
    },
    {
      "metadata": {
        "id": "1ChDPvgoTdVb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "\n",
        "tbc=TensorBoardColab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dM0GXNmxDn5x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Execution Session"
      ]
    },
    {
      "metadata": {
        "id": "hPfE5jURDtF5",
        "colab_type": "code",
        "outputId": "c73912d7-5962-4f12-fac6-4a67a340ba5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "cell_type": "code",
      "source": [
        "logdir1 = \"./Graph/RNN_Model(training)/\"\n",
        "logdir2 = \"./Graph/RNN_Model(testing)/\"\n",
        "\n",
        "epochs=100\n",
        "training_iteration=79 \n",
        "testing_iteration=19\n",
        "\n",
        "\n",
        "with tf.Session(graph=RNN_graph) as sess:\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())   \n",
        "    writer1 = tf.summary.FileWriter(logdir1, sess.graph)\n",
        "    writer2 = tf.summary.FileWriter(logdir2, sess.graph)\n",
        "    \n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "        \n",
        "    for j in range(epochs):\n",
        "      \n",
        "        accuracy=[]\n",
        "\n",
        "        loss=[]\n",
        "        \n",
        "        for i in range(training_iteration):\n",
        "                                    \n",
        "            feed_dict={typhoon_X:training_features[i], typhoon_ytrue:training_labels[i]}\n",
        "            \n",
        "            sess.run([rnn_opt],feed_dict=feed_dict)                    \n",
        "            loss_value,accuracy_value=sess.run([rnn_loss,rnn_accuracy],feed_dict=feed_dict)\n",
        "            \n",
        "                 \n",
        "            \n",
        "            accuracy.append(accuracy_value)\n",
        "            loss.append(loss_value)\n",
        "                \n",
        "        value = tf.Summary.Value(tag=\"Loss\",simple_value=np.mean(loss))\n",
        "        summary = tf.Summary(value=[value])\n",
        "        writer1.add_summary(summary,j)\n",
        "            \n",
        "        value = tf.Summary.Value(tag=\"Accuracy\",simple_value=np.mean(accuracy))\n",
        "        summary = tf.Summary(value=[value])\n",
        "        writer1.add_summary(summary,j)\n",
        "\n",
        "        # Evaluating testdata\n",
        "        accuracy=[]\n",
        "        precision=[]\n",
        "        recall=[]\n",
        "        f1=[]\n",
        "        \n",
        "        for i in range(testing_iteration):\n",
        "                                    \n",
        "            feed_dict={typhoon_X:testing_features[i], typhoon_ytrue:testing_labels[i]}                        \n",
        "            \n",
        "            loss_value,accuracy_value,typhoon_ypred_value=sess.run([rnn_loss,rnn_accuracy,y_p],feed_dict=feed_dict)\n",
        "            \n",
        "            accuracy.append(accuracy_value)\n",
        "            loss.append(loss_value)\n",
        "            \n",
        "            \n",
        "            #metrics\n",
        "            y_true = np.argmax(testing_labels[i],1)\n",
        "            \n",
        "            accuracy.append(accuracy_value)\n",
        "            precision.append(precision_score(y_true, typhoon_ypred_value,average='weighted'))\n",
        "            recall.append(recall_score(y_true, typhoon_ypred_value,average='weighted'))\n",
        "            f1.append(f1_score(y_true, typhoon_ypred_value,average='weighted'))\n",
        "            \n",
        "            \n",
        "        \n",
        "                                \n",
        "        value = tf.Summary.Value(tag=\"Loss\",simple_value=np.mean(loss))\n",
        "        summary = tf.Summary(value=[value])\n",
        "        writer2.add_summary(summary,j)\n",
        "                    \n",
        "        value = tf.Summary.Value(tag=\"Accuracy\",simple_value=np.mean(accuracy))\n",
        "        summary2 = tf.Summary(value=[value])\n",
        "        writer2.add_summary(summary2,j)\n",
        "      \n",
        "      \n",
        "      \n",
        "      # ----- Printing Average Performance Results After Training 100 Epoches------------------------# \n",
        "    print (\"Performance metric:\",np.mean(accuracy),np.mean(precision),np.mean(recall),np.mean(f1))        \n",
        "        \n",
        "      \n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Performance metric: 0.7763158 0.8172748766154161 0.7763157894736842 0.7671323709516361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "itm6wceYMHAO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluate Joint-Model (LSTM+RNN)\n"
      ]
    },
    {
      "metadata": {
        "id": "1D2LCv64Y8he",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Feature Extractor Model (LSTM)"
      ]
    },
    {
      "metadata": {
        "id": "bfPGVU9eMJP_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lstm_size = 64\n",
        "senti_batch_size = 48\n",
        "learning_rate = 0.001\n",
        "senti_classes=2 \n",
        "embed_size=200\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fZ_YzQsMRe0R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def LSTM_Model(senti_inputs,transfer_learning=False):\n",
        "  \n",
        "    data=tf.nn.embedding_lookup(wordVectors,senti_inputs) # tweets_model_wordVectors, word_embedding_vectors, gloveVectors, wordVectors\n",
        "        \n",
        "    lstmCell=tf.nn.rnn_cell.LSTMCell(num_units=lstm_size,name='basic_lstm_cell')\n",
        "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
        "    outputs, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
        "\n",
        "    weight = tf.Variable(tf.truncated_normal([lstm_size, senti_classes]))\n",
        "    bias = tf.Variable(tf.constant(0.1, shape=[senti_classes]))\n",
        "    \n",
        "    value = tf.transpose(outputs, [1, 0, 2])\n",
        "    last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
        "    senti_logits = (tf.matmul(last, weight) + bias)\n",
        "    \n",
        "    senti_predictions=tf.nn.softmax(senti_logits)\n",
        "    \n",
        "    return senti_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fs6fcGvDZ3kx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1 Loading Data for Feature Extractor Model"
      ]
    },
    {
      "metadata": {
        "id": "vOaZCXHcaAXZ",
        "colab_type": "code",
        "outputId": "0aede2f3-1018-4ce4-97db-b2732c4420b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "# Loading word embeddiing vocabs\n",
        "words=list(word_emb.wv.vocab)\n",
        "wordVectors=word_emb[word_emb.wv.vocab]\n",
        "\n",
        "maxSeqLength = 20 # based on average count of words per tweets in training dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "0_kmwsgHacuK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Buidling words indices.\n",
        "words_indices={} # from tweets2vec model\n",
        "\n",
        "i=0\n",
        "for w in words:\n",
        "  words_indices[w]=i\n",
        "  i+=1\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5GBw30p4ajzZ",
        "colab_type": "code",
        "outputId": "1b333c9f-7eb3-4eb7-986d-9e8cd92e5d8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "cell_type": "code",
      "source": [
        "sentiment_features = np.zeros((len(x_sentiment), maxSeqLength), dtype=int)\n",
        "\n",
        "i=0 # instance counter\n",
        "for instance in tqdm(x_sentiment):    \n",
        "    vectors=np.zeros(maxSeqLength,dtype=int)\n",
        "   \n",
        "    j=0  # word counter\n",
        "    for word in instance[:maxSeqLength]:\n",
        "        if word in words:\n",
        "            vectors[j]=words_indices[word] #get word index\n",
        "            j+=1\n",
        "    \n",
        "    sentiment_features[i]=vectors\n",
        "    i+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1600000/1600000 [13:56<00:00, 1913.06it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "L2xanyX5awbc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encode label classes into one-hot vector"
      ]
    },
    {
      "metadata": {
        "id": "ilIamtzMa39j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# representing class data as one-hot vectors\n",
        "y_sentiment_ = np.array([y_sentiment]).reshape(-1)\n",
        "one_hot_targets = np.eye(2)[y_sentiment_]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "beXUNlG3bFdb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3 Split Data (train-test, 80%-20%)"
      ]
    },
    {
      "metadata": {
        "id": "YlMJ8r1ebAI7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split data to train and test\n",
        "split_frac = 0.8\n",
        "split_idx = int(len(sentiment_features)*split_frac)\n",
        "\n",
        "senti_train_x, senti_val_x = sentiment_features[:split_idx], sentiment_features[split_idx:]\n",
        "senti_train_y, senti_val_y = one_hot_targets[:split_idx], one_hot_targets[split_idx:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N2FA8uk-bU_W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.4 Load Batches of Sentiment140 dataset"
      ]
    },
    {
      "metadata": {
        "id": "V-y84X2pbX1F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_Sentiment_Training_Batch():\n",
        "  \n",
        "    labels = np.zeros([senti_batch_size,senti_classes], dtype=int)\n",
        "    arr = np.zeros([senti_batch_size, maxSeqLength], dtype=int)\n",
        "    \n",
        "    for i in range(senti_batch_size):\n",
        "        num = randint(1,len(senti_train_x)-1) # get random index\n",
        "        \n",
        "        labels[i]=senti_train_y[num]\n",
        "        arr[i] = sentiment_features[num]\n",
        "    return arr, labels\n",
        "\n",
        "def load_Sentiment_Testing_Batch():\n",
        "  \n",
        "    labels = np.zeros([senti_batch_size,senti_classes], dtype=int)\n",
        "    arr = np.zeros([senti_batch_size, maxSeqLength],dtype=int)\n",
        "    \n",
        "    for i in range(senti_batch_size):\n",
        "        num = randint(1,len(senti_val_x)-1) # get random index\n",
        "        \n",
        "        labels[i]=senti_val_y[num]\n",
        "        arr[i] = sentiment_features[num]\n",
        "    \n",
        "    return arr, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1juCkQu-c3aM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Joint Model (LSTM+RNN) - Load Training and Testing dataset"
      ]
    },
    {
      "metadata": {
        "id": "7-fTDkMNdCmi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_training_Batches(n=79): \n",
        "    typhoon_features=[]\n",
        "    labels=[]\n",
        "    all_aggreTweets=[]\n",
        "    tweetsCounts=[]\n",
        "    \n",
        "    for i in range(n):\n",
        "        \n",
        "        typhoon_features.append(np.load('./Training Data-Joint Model/typhoon_features/typhoon_features'+str(i)+'.npy'))\n",
        "        labels.append(np.int_(np.load('./Training Data-Joint Model/labels/labels'+str(i)+'.npy')))\n",
        "        \n",
        "        all_aggreTweets.append(np.int_(np.load('./Training Data-Joint Model/all_aggreTweets/all_aggreTweets'+str(i)+'.npy')))\n",
        "        tweetsCounts.append(np.int_(np.load('./Training Data-Joint Model/tweetsCounts/tweetsCounts'+str(i)+'.npy')))\n",
        "    \n",
        "    return typhoon_features,labels,all_aggreTweets,tweetsCounts\n",
        "  \n",
        "  \n",
        "def load_testing_Batches(n=19): \n",
        "    typhoon_features=[]\n",
        "    labels=[]\n",
        "    all_aggreTweets=[]\n",
        "    tweetsCounts=[]\n",
        "    \n",
        "    for i in range(n):\n",
        "        \n",
        "        typhoon_features.append(np.load('./Testing Data-Joint Model/typhoon_features/typhoon_features'+str(i)+'.npy'))\n",
        "        labels.append(np.int_(np.load('./Testing Data-Joint Model/labels/labels'+str(i)+'.npy')))\n",
        "        \n",
        "        all_aggreTweets.append(np.int_(np.load('./Testing Data-Joint Model/all_aggreTweets/all_aggreTweets'+str(i)+'.npy')))\n",
        "        tweetsCounts.append(np.int_(np.load('./Testing Data-Joint Model/tweetsCounts/tweetsCounts'+str(i)+'.npy')))\n",
        "    \n",
        "    return typhoon_features,labels,all_aggreTweets,tweetsCounts\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3QDwnhXSdU1k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_features,training_labels,Tweets_train,tweetsCounts_train=load_training_Batches()\n",
        "testing_features,testing_labels,Tweets_test,tweetsCounts_test=load_testing_Batches()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cPsoU3VsfsAQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize Training and Testing in Tensorboard"
      ]
    },
    {
      "metadata": {
        "id": "E4tvIRXqfxuJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
        "\n",
        "tbc=TensorBoardColab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kin_X9sgZB6H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Joint-Model (LSTM+RNN) - Computation Graph"
      ]
    },
    {
      "metadata": {
        "id": "xO93_okHZD8t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "LSTM_RNN_Graph=tf.Graph()\n",
        "\n",
        "with LSTM_RNN_Graph.as_default():\n",
        "    \n",
        "    with tf.name_scope('Tweets_Placeholders') as scope:\n",
        "        tweets_inputs=tf.placeholder(tf.int32,[None,maxSeqLength],name='AllTweets_input')\n",
        "        tweets_labels=tf.placeholder(tf.int32,[senti_batch_size,senti_classes],name='sentiment_labels')\n",
        "        tweets_Sizes=tf.placeholder(tf.int32,[typhoon_batchSize],name='AggreTweets_Sizes')\n",
        "    \n",
        "    with tf.name_scope('Typhoon_Placeholder') as scope:          \n",
        "        typhoon_X=tf.placeholder(name='Typhoon_Input',dtype=tf.float32,shape=[None,None])\n",
        "        typhoon_ytrue=tf.placeholder(name='Typhoon_Label',dtype=tf.float32,shape=[None,typhoon_classes])\n",
        "        \n",
        "    with tf.device('/gpu:0'):\n",
        "        with tf.name_scope('LSTM_Model') as scope:\n",
        "            \n",
        "            tweets_pred=LSTM_Model(tweets_inputs,transfer_learning=False)\n",
        "            \n",
        "            sentiTweets_pred=tweets_pred[:senti_batch_size] \n",
        "            typhoonTweet_pred=tweets_pred[senti_batch_size:]\n",
        "\n",
        "            lstm_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=sentiTweets_pred,labels=tweets_labels)) \n",
        "\n",
        "    with tf.name_scope('Tweets_Statistics') as scope:\n",
        "            # Tweets aggrgation and Analysis\n",
        "            def condition(tweets_Sizes,typhoonTweet_pred,typhoon_x,newTyphoon_x,j,i):            \n",
        "                return tf.less(i, tf.shape(tweets_Sizes)[0])\n",
        "\n",
        "            def body(tweets_Sizes,typhoonTweet_pred,typhoon_x,newTyphoon_x,j,i):\n",
        "\n",
        "                # get prediction of one typhoon at a time; here we do slicing to typhoonTweet_pred tensor\n",
        "                # i: index of aggreTweet size in Tweets_size tensor; while j step-by index in predictions tensor\n",
        "                typhoon_prediction=typhoonTweet_pred[j:tweets_Sizes[i]] \n",
        "\n",
        "                tweetsCount=tf.cast(tf.nn.embedding_lookup(tweets_Sizes,i),tf.float32)\n",
        "\n",
        "                # get mean and variance\n",
        "                _, variance = tf.nn.moments(typhoonTweet_pred, [0])    \n",
        "                std=tf.sqrt(variance)\n",
        "                tweets_features=tf.concat([[tweetsCount],std],0)\n",
        "                tweets_features_Normalized=tf.nn.l2_normalize(tweets_features,dim=0)\n",
        "                tweets_features_Normalized=tweets_features\n",
        "\n",
        "                # Get Current typhoon instance from Typhoon batch:\n",
        "                current_Typhoon=tf.nn.embedding_lookup(typhoon_x,i)\n",
        "                # extend tweets feature with typhoon     \n",
        "                new_Typhoon=tf.concat([current_Typhoon,tweets_features_Normalized],0)\n",
        "                newTyphoon_x=newTyphoon_x.write(i,new_Typhoon)\n",
        "\n",
        "                return tweets_Sizes,typhoonTweet_pred,typhoon_x,newTyphoon_x,tweets_Sizes[i],i+1\n",
        "\n",
        "            newTyphoon_x= tf.TensorArray(dtype=tf.float32,size=0,dynamic_size=True,element_shape=(featuresCount,))    \n",
        "            _,_,_,newTyphoon_x,_,_=tf.while_loop(condition, body, [tweets_Sizes,typhoonTweet_pred,typhoon_X,newTyphoon_x, 0,0])       \n",
        "            newTyphoon_x = newTyphoon_x.stack()\n",
        "            \n",
        "    with tf.device('/gpu:0'):\n",
        "        with tf.name_scope('RNN_Model') as scope:\n",
        "          \n",
        "          update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "          \n",
        "          with tf.control_dependencies(update_ops): \n",
        "            \n",
        "            typhoon_ypred=RNN_model(newTyphoon_x)                          \n",
        "\n",
        "            # Ensures that we execute the update_ops before performing the train_step            \n",
        "            rnn_loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=typhoon_ypred,labels=typhoon_ytrue))                       \n",
        "              # joint training loss function              \n",
        "            joint_Loss=lstm_loss+rnn_loss  \n",
        "                        \n",
        "                        \n",
        "            #Cliping the gradient loss\n",
        "            gradients = tf.gradients(joint_Loss, tf.trainable_variables())\n",
        "            clipper_, _ = tf.clip_by_global_norm(gradients, grad_clip_margin)\n",
        "            \n",
        "            optimizer = tf.train.AdamOptimizer()\n",
        "            rnn_opt = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
        "            \n",
        "            predict_opt=tf.equal(tf.argmax(typhoon_ypred,1), tf.argmax(typhoon_ytrue,1))\n",
        "            joint_accuracy = tf.reduce_mean(tf.cast(predict_opt, tf.float32))\n",
        "            \n",
        "            y_p=tf.argmax(typhoon_ypred,1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hRmSFbRjcUnQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Joint Model (LSTM+RNN) - Execution Session"
      ]
    },
    {
      "metadata": {
        "id": "0cQsHlG5cbDg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logdir1 = \"./Graph/LSTM-RNN(training)\"\n",
        "logdir2 = \"./Graph/LSTM-RNN(test)\"\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth=True\n",
        "config.allow_soft_placement=True\n",
        "config.log_device_placement=True\n",
        "\n",
        "epochs=100\n",
        "training_iteration=79 \n",
        "testing_iteration=19\n",
        "\n",
        "\n",
        "with tf.Session(graph=LSTM_RNN_Graph,config=config) as sess:\n",
        "    sess.run(tf.global_variables_initializer())   \n",
        "    \n",
        "    writer1 = tf.summary.FileWriter(logdir1, sess.graph)\n",
        "    writer2 = tf.summary.FileWriter(logdir2, sess.graph)\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "        \n",
        "    for j in range(epochs):\n",
        "        \n",
        "        \n",
        "        accuracy=[]\n",
        "                      \n",
        "        loss=[]\n",
        "        \n",
        "        for i in range(training_iteration):\n",
        "            \n",
        "            senti_x,senti_y=load_Sentiment_Training_Batch()            \n",
        "            all_tweets=np.concatenate((senti_x,Tweets_train[i]),axis=0)\n",
        "            \n",
        "            feed_dict={typhoon_X:training_features[i],typhoon_ytrue:training_labels[i],\n",
        "                       tweets_inputs:all_tweets,tweets_labels:senti_y,tweets_Sizes:tweetsCounts_train[i]}\n",
        "\n",
        "            sess.run([rnn_opt],feed_dict=feed_dict)\n",
        "            \n",
        "            loss_value,accuracy_value=sess.run([joint_Loss,joint_accuracy],feed_dict=feed_dict)\n",
        "                      \n",
        "            \n",
        "            accuracy.append(accuracy_value)\n",
        "            loss.append(loss_value)\n",
        "                                \n",
        "        value = tf.Summary.Value(tag=\"Loss\",simple_value=np.mean(loss))\n",
        "        summary = tf.Summary(value=[value])\n",
        "        writer1.add_summary(summary,j)\n",
        "          \n",
        "                                                                               \n",
        "        value = tf.Summary.Value(tag=\"Accuracy\",simple_value=np.mean(accuracy))\n",
        "        summary = tf.Summary(value=[value])\n",
        "        writer1.add_summary(summary,j)\n",
        "\n",
        "        # Evaluating testdata\n",
        "        accuracy=[]\n",
        "        loss=[]\n",
        "        precision=[]\n",
        "        recall=[]\n",
        "        f1=[]\n",
        "            \n",
        "        for i in range(testing_iteration):\n",
        "                    \n",
        "          senti_x,senti_y=load_Sentiment_Testing_Batch()\n",
        "            \n",
        "          all_tweets=np.concatenate((senti_x,Tweets_test[i]),axis=0)\n",
        "          \n",
        "          feed_dict={typhoon_X:testing_features[i],typhoon_ytrue:testing_labels[i],\n",
        "                   tweets_inputs:all_tweets,tweets_labels:senti_y,tweets_Sizes:tweetsCounts_test[i]}\n",
        "          \n",
        "          loss_value,accuracy_value,typhoon_ypred_value=sess.run([joint_Loss,joint_accuracy,y_p],feed_dict=feed_dict)\n",
        "          \n",
        "          accuracy.append(accuracy_value)\n",
        "          loss.append(loss_value)\n",
        "          \n",
        "     \n",
        "          y_true = np.argmax(testing_labels[i],1)\n",
        "          \n",
        "          precision.append(precision_score(y_true, typhoon_ypred_value,average='weighted'))\n",
        "          recall.append(recall_score(y_true, typhoon_ypred_value,average='weighted'))\n",
        "          f1.append(f1_score(y_true, typhoon_ypred_value,average='weighted'))\n",
        "\n",
        "          \n",
        "        value = tf.Summary.Value(tag=\"Loss\",simple_value=np.mean(loss))\n",
        "        summary = tf.Summary(value=[value])\n",
        "        writer2.add_summary(summary,j)\n",
        "                    \n",
        "        value = tf.Summary.Value(tag=\"Accuracy\",simple_value=np.mean(accuracy))\n",
        "        summary2 = tf.Summary(value=[value])\n",
        "        writer2.add_summary(summary2,j)\n",
        "        \n",
        "        \n",
        "    print (\"Performance Results (Accuracy, Precision, Recall and F1-Score):\",np.mean(accuracy),np.mean(precision),np.mean(recall),np.mean(f1))                     \n",
        "    \n",
        "    writer1.close() \n",
        "    writer2.close()     "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}